from __future__ import annotations
import torch
import os
import tempfile
import io
import torchaudio
from transformers import Qwen2_5OmniForConditionalGeneration, AutoProcessor, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import snapshot_download
from modelscope.hub.snapshot_download import snapshot_download as modelscope_snapshot_download
from PIL import Image
from pathlib import Path
import folder_paths
from qwen_omni_utils import process_mm_info
import numpy as np
import soundfile as sf
import requests
import time
from .VideoUploader import VideoUploader
import torchvision


# æ¨¡å‹æ³¨å†Œè¡¨ - å­˜å‚¨æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬ä¿¡æ¯
# Model Registry - Stores information about all supported model versions
MODEL_REGISTRY = {
    "Qwen2.5-Omni-3B": {
        "repo_id": {
            "huggingface": "Qwen/Qwen2.5-Omni-3B",
            "modelscope": "qwen/Qwen2.5-Omni-3B"
        },
        "required_files": [
            "added_tokens.json", "chat_template.json", "merges.txt",
            "model.safetensors.index.json", "preprocessor_config.json", 
            "spk_dict.pt", "tokenizer.json", "vocab.json", "config.json",
            "generation_config.json", "special_tokens_map.json",
            "tokenizer_config.json",
            # 3Bæ¨¡å‹åˆ†ç‰‡ä¸º3ä¸ª
            # 3B model is split into 3 shards
            "model-00001-of-00003.safetensors",
            "model-00002-of-00003.safetensors",
            "model-00003-of-00003.safetensors",
        ],
        "test_file": "model-00003-of-00003.safetensors",
        "default": True
    },
    "Qwen2.5-Omni-7B": {
        "repo_id": {
            "huggingface": "Qwen/Qwen2.5-Omni-7B",
            "modelscope": "qwen/Qwen2.5-Omni-7B"
        },
        "required_files": [
            "added_tokens.json", "chat_template.json", "merges.txt",
            "model.safetensors.index.json", "preprocessor_config.json", 
            "spk_dict.pt", "tokenizer.json", "vocab.json", "config.json",
            "generation_config.json", "special_tokens_map.json",
            "tokenizer_config.json",
            # 7Bæ¨¡å‹æœ‰5ä¸ªåˆ†ç‰‡
            # 7B model has 5 shards
            "model-00001-of-00005.safetensors",
            "model-00002-of-00005.safetensors",
            "model-00003-of-00005.safetensors",
            "model-00004-of-00005.safetensors",
            "model-00005-of-00005.safetensors",
        ],
        "test_file": "model-00005-of-00005.safetensors",
        "default": False
    }
}


def check_flash_attention():
    """
    æ£€æµ‹Flash Attention 2æ”¯æŒï¼ˆéœ€Ampereæ¶æ„åŠä»¥ä¸Šï¼‰
    Check Flash Attention 2 support (requires Ampere architecture or higher)
    """
    try:
        from flash_attn import flash_attn_func
        major, _ = torch.cuda.get_device_capability()
        return major >= 8  # ä»…æ”¯æŒè®¡ç®—èƒ½åŠ›8.0+çš„GPU
        # Only supports GPUs with compute capability 8.0+
    except ImportError:
        return False


FLASH_ATTENTION_AVAILABLE = check_flash_attention()


def init_qwen_paths(model_name):
    """
    åˆå§‹åŒ–æ¨¡å‹è·¯å¾„ï¼Œæ”¯æŒåŠ¨æ€ç”Ÿæˆä¸åŒæ¨¡å‹ç‰ˆæœ¬çš„è·¯å¾„
    Initialize model paths, supporting dynamic generation of paths for different model versions
    """
    base_dir = Path(folder_paths.models_dir).resolve()
    qwen_dir = base_dir / "Qwen"
    model_dir = qwen_dir / model_name  # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºå­ç›®å½•
    # Use model name as subdirectory
    
    # åˆ›å»ºç›®å½•
    # Create directories
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # æ³¨å†Œåˆ°ComfyUI
    # Register to ComfyUI
    if hasattr(folder_paths, "add_model_folder_path"):
        folder_paths.add_model_folder_path("Qwen", str(model_dir))
    else:
        folder_paths.folder_names_and_paths["Qwen"] = ([str(model_dir)], {'.safetensors', '.bin'})
    
    print(f"æ¨¡å‹è·¯å¾„å·²åˆå§‹åŒ–: {model_dir}")
    print(f"Model path initialized: {model_dir}")
    return str(model_dir)


def test_download_speed(url):
    """
    æµ‹è¯•ä¸‹è½½é€Ÿåº¦ï¼Œä¸‹è½½ 5 ç§’
    Test download speed by downloading for 5 seconds
    """
    try:
        start_time = time.time()
        response = requests.get(url, stream=True, timeout=10)
        downloaded_size = 0
        for data in response.iter_content(chunk_size=1024):
            if time.time() - start_time > 5:
                break
            downloaded_size += len(data)
        end_time = time.time()
        speed = downloaded_size / (end_time - start_time) / 1024  # KB/s
        return speed
    except Exception as e:
        print(f"æµ‹è¯•ä¸‹è½½é€Ÿåº¦æ—¶å‡ºç°é”™è¯¯: {e}")
        print(f"Error occurred while testing download speed: {e}")
        return 0


def validate_model_path(model_path, model_name):
    """
    éªŒè¯æ¨¡å‹è·¯å¾„çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨
    Validate the validity of the model path and whether all model files are present
    """
    path_obj = Path(model_path)
    
    # åŸºæœ¬è·¯å¾„æ£€æŸ¥
    # Basic path check
    if not path_obj.is_absolute():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç»å¯¹è·¯å¾„")
        print(f"Error: {model_path} is not an absolute path")
        return False
    
    if not path_obj.exists():
        print(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
        print(f"Model directory does not exist: {model_path}")
        return False
    
    if not path_obj.is_dir():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç›®å½•")
        print(f"Error: {model_path} is not a directory")
        return False
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨
    # Check if all required model files are present
    if not check_model_files_exist(model_path, model_name):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´: {model_path}")
        print(f"Model files are incomplete: {model_path}")
        return False
    
    return True


def check_model_files_exist(model_dir, model_name):
    """
    æ£€æŸ¥ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬æ‰€éœ€çš„æ–‡ä»¶æ˜¯å¦é½å…¨
    Check if all files required for a specific model version are present
    """
    if model_name not in MODEL_REGISTRY:
        print(f"é”™è¯¯: æœªçŸ¥æ¨¡å‹ç‰ˆæœ¬ {model_name}")
        print(f"Error: Unknown model version {model_name}")
        return False
    
    required_files = MODEL_REGISTRY[model_name]["required_files"]
    for file in required_files:
        if not os.path.exists(os.path.join(model_dir, file)):
            return False
    return True


# è§†é¢‘å¤„ç†å·¥å…·ç±»
# Video Processing Utility Class
class VideoProcessor:
    def __init__(self):
        # å°è¯•å¯¼å…¥torchcodecä½œä¸ºå¤‡é€‰è§†é¢‘å¤„ç†åº“
        # Try to import torchcodec as an alternative video processing library
        self.use_torchcodec = False
        try:
            import torchcodec
            self.use_torchcodec = True
            print("ä½¿ç”¨torchcodecè¿›è¡Œè§†é¢‘å¤„ç†")
            print("Using torchcodec for video processing")
        except ImportError:
            print("torchcodecä¸å¯ç”¨ï¼Œä½¿ç”¨torchvisionè¿›è¡Œè§†é¢‘å¤„ç†")
            print("torchcodec is unavailable, using torchvision for video processing")
            # æŠ‘åˆ¶torchvisionè§†é¢‘APIå¼ƒç”¨è­¦å‘Š
            # Suppress torchvision video API deprecation warnings
            import warnings
            warnings.filterwarnings("ignore", category=UserWarning, module="torchvision.io")
    
    def read_video(self, video_path):
        """
        è¯»å–è§†é¢‘æ–‡ä»¶å¹¶è¿”å›å¸§æ•°æ®
        Read video file and return frame data
        """
        start_time = time.time()
        try:
            if self.use_torchcodec:
                # ä½¿ç”¨torchcodecè¯»å–è§†é¢‘
                # Read video using torchcodec
                import torchcodec
                decoder = torchcodec.VideoDecoder(video_path)
                frames = []
                for frame in decoder:
                    frames.append(frame)
                fps = decoder.get_fps()
                total_frames = len(frames)
                frames = torch.stack(frames) if frames else torch.zeros(0)
            else:
                # ä½¿ç”¨torchvisionè¯»å–è§†é¢‘ï¼ˆå¼ƒç”¨APIï¼‰
                # Read video using torchvision (deprecated API)
                frames, _, info = torchvision.io.read_video(video_path, pts_unit="sec")
                fps = info["video_fps"]
                total_frames = frames.shape[0]
            
            process_time = time.time() - start_time
            print(f"è§†é¢‘å¤„ç†å®Œæˆ: {video_path}, æ€»å¸§æ•°: {total_frames}, FPS: {fps:.2f}, å¤„ç†æ—¶é—´: {process_time:.3f}s")
            print(f"Video processing completed: {video_path}, Total frames: {total_frames}, FPS: {fps:.2f}, Processing time: {process_time:.3f}s")
            return frames, fps, total_frames
            
        except Exception as e:
            print(f"è§†é¢‘å¤„ç†é”™è¯¯: {e}")
            print(f"Video processing error: {e}")
            return None, None, None


class QwenOmniCombined:
    def __init__(self):
        # é»˜è®¤ä½¿ç”¨æ³¨å†Œè¡¨ä¸­çš„ç¬¬ä¸€ä¸ªé»˜è®¤æ¨¡å‹
        # Use the first default model in the registry by default
        default_model = next((name for name, info in MODEL_REGISTRY.items() if info.get("default", False)), 
                            list(MODEL_REGISTRY.keys())[0])
        
        # é‡ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…å¹²æ‰°
        # Reset environment variables to avoid interference
        os.environ.pop("HUGGINGFACE_HUB_CACHE", None)     

        self.current_model_name = default_model
        self.current_quantization = None  # è®°å½•å½“å‰çš„é‡åŒ–é…ç½®
        # Record current quantization configuration
        self.model_path = init_qwen_paths(self.current_model_name)
        self.cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"Model path: {self.model_path}")
        print(f"ç¼“å­˜è·¯å¾„: {self.cache_dir}")
        print(f"Cache path: {self.cache_dir}")
        
        # éªŒè¯å¹¶åˆ›å»ºç¼“å­˜ç›®å½•
        # Validate and create cache directory
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)

        self.model = None
        self.processor = None
        self.tokenizer = None
        self.video_processor = VideoProcessor()  # åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨
        # Initialize video processor
        self.last_generated_text = ""  # ä¿å­˜ä¸Šæ¬¡ç”Ÿæˆçš„æ–‡æœ¬ï¼Œç”¨äºè°ƒè¯•
        # Save last generated text for debugging
        self.generation_stats = {"count": 0, "total_time": 0}  # ç»Ÿè®¡ç”Ÿæˆæ€§èƒ½
        # Statistics for generation performance

    def clear_model_resources(self):
        """
        é‡Šæ”¾å½“å‰æ¨¡å‹å ç”¨çš„èµ„æº
        Release resources occupied by the current model
        """
        if self.model is not None:
            print("é‡Šæ”¾å½“å‰æ¨¡å‹å ç”¨çš„èµ„æº...")
            print("Releasing resources occupied by the current model...")
            del self.model, self.processor, self.tokenizer
            self.model = None
            self.processor = None
            self.tokenizer = None
            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
            # Clean GPU cache
    
    def load_model(self, model_name, quantization):
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹
        # Check if model needs to be reloaded
        if (self.model is not None and 
            self.current_model_name == model_name and 
            self.current_quantization == quantization):
            print(f"ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹: {model_name}ï¼Œé‡åŒ–: {quantization}")
            print(f"Using already loaded model: {model_name}, Quantization: {quantization}")
            return
        
        # éœ€è¦é‡æ–°åŠ è½½ï¼Œå…ˆé‡Šæ”¾ç°æœ‰èµ„æº
        # Need to reload, release existing resources first
        self.clear_model_resources()
        
        # æ›´æ–°å½“å‰æ¨¡å‹åç§°å’Œè·¯å¾„
        # Update current model name and path
        self.current_model_name = model_name
        self.model_path = init_qwen_paths(self.current_model_name)
        self.current_quantization = quantization
        
        # æ·»åŠ CUDAå¯ç”¨æ€§æ£€æŸ¥
        # Add CUDA availability check
        if not torch.cuda.is_available():
            raise RuntimeError(f"CUDA is required for  {model_name} model")
            # ä¸­æ–‡æç¤º: è¿è¡Œ {model_name} æ¨¡å‹éœ€è¦CUDAæ”¯æŒ
            # Chinese prompt: CUDA support is required to run the {model_name} model

        # æ·»åŠ è­¦å‘Šè¿‡æ»¤
        # Add warning filter
        import warnings
        warnings.filterwarnings("ignore", category=UserWarning, message="MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization")

        quant_config = None
        compute_dtype = torch.float16  # é»˜è®¤ä½¿ç”¨float16
        # Use float16 by default
        if quantization == "ğŸ‘ 4-bit (VRAM-friendly)":
            quant_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=compute_dtype,  # ç¡®ä¿è®¡ç®—ç²¾åº¦ä¸ºfloat16
                # Ensure computation precision is float16
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
        elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
            quant_config = BitsAndBytesConfig(
                load_in_8bit=True,
                bnb_8bit_compute_dtype=compute_dtype  # ç¡®ä¿è®¡ç®—ç²¾åº¦ä¸ºfloat16
                # Ensure computation precision is float16
            )

        # è‡ªå®šä¹‰device_mapï¼Œè¿™é‡Œå‡è®¾åªæœ‰ä¸€ä¸ªGPUï¼Œå°†æ¨¡å‹å°½å¯èƒ½æ”¾åˆ°GPUä¸Š
        # Custom device_map, assuming there's only one GPU here, place the model on the GPU as much as possible
        device_map = {"": 0} if torch.cuda.device_count() > 0 else "auto"

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
        # Check if model files exist and are complete
        if not validate_model_path(self.model_path, self.current_model_name):
            print(f"æ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œæ­£åœ¨ä¸ºä½ ä¸‹è½½ {model_name} æ¨¡å‹ï¼Œè¯·ç¨å€™...")
            print(f"Missing model files detected, downloading {model_name} model for you, please wait...")
            print(f"ä¸‹è½½å°†ä¿å­˜åœ¨: {self.model_path}")
            print(f"Download will be saved to: {self.model_path}")
            
            # å¼€å§‹ä¸‹è½½é€»è¾‘
            # Start download logic
            try:
                # ä»æ³¨å†Œè¡¨è·å–æ¨¡å‹ä¿¡æ¯
                # Get model information from registry
                model_info = MODEL_REGISTRY[model_name]
                
                # æµ‹è¯•ä¸‹è½½é€Ÿåº¦
                # Test download speed
                huggingface_test_url = f"https://huggingface.co/{model_info['repo_id']['huggingface']}/resolve/main/{model_info['test_file']}"
                modelscope_test_url = f"https://modelscope.cn/api/v1/models/{model_info['repo_id']['modelscope']}/repo?Revision=master&FilePath={model_info['test_file']}"
                huggingface_speed = test_download_speed(huggingface_test_url)
                modelscope_speed = test_download_speed(modelscope_test_url)

                print(f"Hugging Faceä¸‹è½½é€Ÿåº¦: {huggingface_speed:.2f} KB/s")
                print(f"Hugging Face download speed: {huggingface_speed:.2f} KB/s")
                print(f"ModelScopeä¸‹è½½é€Ÿåº¦: {modelscope_speed:.2f} KB/s")
                print(f"ModelScope download speed: {modelscope_speed:.2f} KB/s")

                # æ ¹æ®ä¸‹è½½é€Ÿåº¦é€‰æ‹©ä¼˜å…ˆä¸‹è½½æº
                # Select priority download source based on download speed
                if huggingface_speed > modelscope_speed * 1.5:
                    download_sources = [
                        (snapshot_download, model_info['repo_id']['huggingface'], "Hugging Face"),
                        (modelscope_snapshot_download, model_info['repo_id']['modelscope'], "ModelScope")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»Hugging Faceä¸‹è½½")
                    print("Based on download speed analysis, trying to download from Hugging Face first")
                else:
                    download_sources = [
                        (modelscope_snapshot_download, model_info['repo_id']['modelscope'], "ModelScope"),
                        (snapshot_download, model_info['repo_id']['huggingface'], "Hugging Face")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»ModelScopeä¸‹è½½")
                    print("Based on download speed analysis, trying to download from ModelScope first")

                max_retries = 3
                success = False
                final_error = None
                used_cache_path = None

                for download_func, repo_id, source in download_sources:
                    for retry in range(max_retries):
                        try:
                            print(f"å¼€å§‹ä» {source} ä¸‹è½½æ¨¡å‹ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰...")
                            print(f"Starting to download model from {source} (Attempt {retry + 1})...")
                            if download_func == snapshot_download:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir,
                                    ignore_patterns=["*.msgpack", "*.h5"],
                                    resume_download=True,
                                    local_files_only=False
                                )
                            else:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir,
                                    revision="master"
                                )

                            used_cache_path = cached_path  # è®°å½•ä½¿ç”¨çš„ç¼“å­˜è·¯å¾„
                            # Record the cache path used
                            
                            # å°†ä¸‹è½½çš„æ¨¡å‹å¤åˆ¶åˆ°æ¨¡å‹ç›®å½•
                            # Copy the downloaded model to the model directory
                            self.copy_cached_model_to_local(cached_path, self.model_path)
                            
                            print(f"æˆåŠŸä» {source} ä¸‹è½½æ¨¡å‹åˆ° {self.model_path}")
                            print(f"Successfully downloaded model from {source} to {self.model_path}")
                            success = True
                            break

                        except Exception as e:
                            final_error = e  # ä¿å­˜æœ€åä¸€ä¸ªé”™è¯¯
                            # Save the last error
                            if retry < max_retries - 1:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå³å°†è¿›è¡Œä¸‹ä¸€æ¬¡å°è¯•...")
                                print(f"Failed to download model from {source} (Attempt {retry + 1}): {e}, proceeding to next attempt...")
                            else:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå°è¯•å…¶ä»–æº...")
                                print(f"Failed to download model from {source} (Attempt {retry + 1}): {e}, trying other source...")
                    if success:
                        break
                else:
                    raise RuntimeError("ä»æ‰€æœ‰æºä¸‹è½½æ¨¡å‹å‡å¤±è´¥ã€‚")
                    # ä¸­æ–‡æç¤º: ä»æ‰€æœ‰æºä¸‹è½½æ¨¡å‹å‡å¤±è´¥ã€‚
                    # Chinese prompt: Failed to download model from all sources.
                
                # ä¸‹è½½å®Œæˆåå†æ¬¡éªŒè¯
                # Validate again after download
                if not validate_model_path(self.model_path, self.current_model_name):
                    raise RuntimeError(f"ä¸‹è½½åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {self.model_path}")
                    # ä¸­æ–‡æç¤º: ä¸‹è½½åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {self.model_path}
                    # Chinese prompt: Model files are still incomplete after download: {self.model_path}
                
                print(f"æ¨¡å‹ {model_name} å·²å‡†å¤‡å°±ç»ª")
                print(f"Model {model_name} is ready")
                
            except Exception as e:
                print(f"ä¸‹è½½æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                print(f"Error occurred while downloading model: {e}")
                
                # ä¸‹è½½å¤±è´¥æç¤º
                # Download failure prompt
                if used_cache_path:
                    print("\nâš ï¸ æ³¨æ„ï¼šä¸‹è½½è¿‡ç¨‹ä¸­åˆ›å»ºäº†ç¼“å­˜æ–‡ä»¶")
                    print("\nâš ï¸ Attention: Cache files were created during the download process")
                    print(f"ç¼“å­˜è·¯å¾„: {used_cache_path}")
                    print(f"Cache path: {used_cache_path}")
                    print("ä½ å¯ä»¥å‰å¾€æ­¤è·¯å¾„åˆ é™¤ç¼“å­˜æ–‡ä»¶ä»¥é‡Šæ”¾ç¡¬ç›˜ç©ºé—´")
                    print("You can go to this path to delete the cache files to free up disk space")
                
                raise RuntimeError(f"æ— æ³•ä¸‹è½½æ¨¡å‹ {model_name}ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ° {self.model_path}")
                # ä¸­æ–‡æç¤º: æ— æ³•ä¸‹è½½æ¨¡å‹ {model_name}ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ° {self.model_path}
                # Chinese prompt: Unable to download model {model_name}, please download manually and place in {self.model_path}

        # æ ¹æ®é‡åŒ–é…ç½®åŠ¨æ€é€‰æ‹©æ³¨æ„åŠ›å®ç°
        # Dynamically select attention implementation based on quantization configuration
        if quant_config is not None:
            # å½“ä½¿ç”¨é‡åŒ–æ—¶ï¼Œå¼ºåˆ¶ä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›å®ç°è€ŒéFlashAttention
            # When using quantization, force standard attention implementation instead of FlashAttention
            attn_impl = "sdpa"
            print("ä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›å®ç° (sdpa) æ›¿ä»£FlashAttentionï¼Œä»¥å…¼å®¹é‡åŒ–æ¨¡å¼")
            print("Using standard attention implementation (sdpa) instead of FlashAttention to support quantization mode")
        else:
            # éé‡åŒ–æ¨¡å¼ä¸‹ï¼Œæ ¹æ®å¯ç”¨æ€§é€‰æ‹©
            # In non-quantization mode, select based on availability
            attn_impl = "flash_attention_2" if FLASH_ATTENTION_AVAILABLE else "sdpa"

        # è®¾ç½®æ¨¡å‹ç²¾åº¦
        # Set model precision
        model_dtype = compute_dtype if quant_config else torch.float16
        # è®°å½•å½“å‰ä½¿ç”¨çš„ç²¾åº¦
        # Record currently used precision
        precision_msg = "fp16" if model_dtype == torch.float16 else "bf16"
        print(f"ä½¿ç”¨ç²¾åº¦: {precision_msg}")        
        print(f"Using precision: {precision_msg}")        
        # æ˜ç¡®è®¾ç½®audioéƒ¨åˆ†çš„ç²¾åº¦ï¼Œç¡®ä¿ä¸æ¨¡å‹å…¶ä»–éƒ¨åˆ†ä¸€è‡´
        # Explicitly set the precision of the audio part to ensure consistency with other parts of the model
        audio_dtype = model_dtype   

        self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            self.model_path,
            device_map=device_map,
            torch_dtype=model_dtype,
            quantization_config=quant_config,
            attn_implementation=attn_impl,
            low_cpu_mem_usage=True,
            use_safetensors=True,
            offload_state_dict=True,
            enable_audio_output=True,
        ).eval()

        # ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.2+ï¼‰
        # Compilation optimization (PyTorch 2.2+)
        if torch.__version__ >= "2.2":
            self.model = torch.compile(self.model, mode="reduce-overhead")

        # SDPä¼˜åŒ–
        # SDP optimization
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)

        self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)

        # ä¿®å¤rope_scalingé…ç½®è­¦å‘Š - ç§»è‡³æ­¤å¤„ç«‹å³æ‰§è¡Œ
        # Fix rope_scaling configuration warning - moved here for immediate execution
        if hasattr(self.model.config, "rope_scaling"):
            print("ä¿®å¤ROPEç¼©æ”¾é…ç½®...")
            print("Fixing ROPE scaling configuration...")
            if "mrope_section" in self.model.config.rope_scaling:
                self.model.config.rope_scaling["mrope_section"] = "none"  # ç¦ç”¨ MROPE ä¼˜åŒ–
                # Disable MROPE optimization
            else:
                print("æ¨¡å‹é…ç½®ä¸­æ²¡æœ‰mrope_sectioné”®ï¼Œæ— éœ€ä¿®å¤")
                print("No mrope_section key in model configuration, no fix needed")

    def copy_cached_model_to_local(self, cached_path, target_path):
        """
        å°†ç¼“å­˜çš„æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°ç›®æ ‡è·¯å¾„
        Copy cached model files to target path
        """
        print(f"æ­£åœ¨å°†æ¨¡å‹ä»ç¼“å­˜å¤åˆ¶åˆ°: {target_path}")
        print(f"Copying model from cache to: {target_path}")
        target_path = Path(target_path)
        target_path.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨shutilè¿›è¡Œé€’å½’å¤åˆ¶
        # Use shutil for recursive copying
        import shutil
        for item in Path(cached_path).iterdir():
            if item.is_dir():
                shutil.copytree(item, target_path / item.name, dirs_exist_ok=True)
            else:
                shutil.copy2(item, target_path / item.name)
        
        # éªŒè¯å¤åˆ¶æ˜¯å¦æˆåŠŸ
        # Validate if copy was successful
        if validate_model_path(target_path, self.current_model_name):
            print(f"æ¨¡å‹å·²æˆåŠŸå¤åˆ¶åˆ° {target_path}")
            print(f"Model successfully copied to {target_path}")
        else:
            raise RuntimeError(f"å¤åˆ¶åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {target_path}")
            # ä¸­æ–‡æç¤º: å¤åˆ¶åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {target_path}
            # Chinese prompt: Model files are still incomplete after copy: {target_path}

    def tensor_to_pil(self, image_tensor):
        """
        å°†å›¾åƒå¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
        Convert image tensor to PIL image
        """
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]
        image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
        return Image.fromarray(image_np)

    def preprocess_image(self, image):
        """
        é¢„å¤„ç†å›¾åƒï¼ŒåŒ…æ‹¬å°ºå¯¸è°ƒæ•´å’Œä¼˜åŒ–
        Preprocess image, including resizing and optimization
        """
        pil_image = self.tensor_to_pil(image)
        
        # é™åˆ¶æœ€å¤§å°ºå¯¸ï¼Œé¿å…è¿‡å¤§çš„è¾“å…¥
        # Limit maximum size to avoid excessively large inputs
        max_res = 1024
        if max(pil_image.size) > max_res:
            pil_image.thumbnail((max_res, max_res))
        
        # è½¬æ¢å›å¼ é‡å¹¶å½’ä¸€åŒ–
        # Convert back to tensor and normalize
        img_np = np.array(pil_image)
        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0) / 255.0
        
        # è½¬å›PILå›¾åƒ
        # Convert back to PIL image
        pil_image = Image.fromarray((img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))
        return pil_image

    def preprocess_video(self, video_path):
        """
        é¢„å¤„ç†è§†é¢‘ï¼ŒåŒ…æ‹¬å¸§æå–å’Œå°ºå¯¸è°ƒæ•´
        Preprocess video, including frame extraction and resizing
        """
        # ä½¿ç”¨è§†é¢‘å¤„ç†å™¨è¯»å–è§†é¢‘
        # Read video using video processor
        frames, fps, total_frames = self.video_processor.read_video(video_path)
        
        if frames is None:
            print(f"æ— æ³•å¤„ç†è§†é¢‘: {video_path}")
            print(f"Unable to process video: {video_path}")
            return None, None, None
        
        # æ›´æ¿€è¿›çš„å¸§æ•°é‡é™åˆ¶
        # More aggressive frame count limit
        max_frames = 15  # ä»50å‡å°‘åˆ°30
        # Reduced from 50 to 30
        if total_frames > max_frames:
            # é‡‡æ ·å¸§
            # Sample frames
            indices = np.linspace(0, total_frames - 1, max_frames, dtype=int)
            frames = frames[indices]
            print(f"è§†é¢‘å¸§æ•°é‡ä» {total_frames} é‡‡æ ·åˆ° {len(frames)}")
            print(f"Video frame count sampled from {total_frames} to {len(frames)}")
        
        # æ›´å°çš„å¸§å°ºå¯¸
        # Smaller frame size
        resized_frames = []
        for frame in frames:
            # è½¬æ¢ä¸ºPILå›¾åƒ
            # Convert to PIL image
            frame_pil = Image.fromarray(frame.numpy())
            # è°ƒæ•´å¤§å°ä¸º384x384 (åŸä¸º512x512)
            # Resize to 384x384 (originally 512x512)
            frame_pil.thumbnail((384, 384))
            # è½¬å›å¼ é‡
            # Convert back to tensor
            frame_tensor = torch.from_numpy(np.array(frame_pil)).permute(2, 0, 1)
            resized_frames.append(frame_tensor)
        
        # è½¬æ¢å›å¼ é‡
        # Convert back to tensor
        if resized_frames:
            resized_frames = torch.stack(resized_frames)
        else:
            resized_frames = torch.zeros(0)
        
        return resized_frames, fps, len(frames)  # è¿”å›å®é™…é‡‡æ ·åçš„å¸§æ•°
        # Return the actual number of sampled frames

    @torch.no_grad()
    def process(self, model_name, quantization, prompt, audio_output, audio_source, max_tokens, temperature, top_p,
                repetition_penalty, audio=None, image=None, video_path=None):
        start_time = time.time()
        
        # ç¡®ä¿åŠ è½½æ­£ç¡®çš„æ¨¡å‹å’Œé‡åŒ–é…ç½®
        # Ensure correct model and quantization configuration are loaded
        self.load_model(model_name, quantization)
        
        # å›¾åƒé¢„å¤„ç†
        # Image preprocessing
        pil_image = None
        if image is not None:
            pil_image = self.preprocess_image(image)
        
        # éŸ³é¢‘é¢„å¤„ç†
        # Audio preprocessing
        audio_path = None
        if audio:
            try:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¹¶ä¿å­˜éŸ³é¢‘
                # Create temporary file and save audio
                with tempfile.NamedTemporaryFile(suffix=".flac", delete=False) as f:
                    audio_path = f.name
                    waveform = audio["waveform"].squeeze(0).cpu().numpy()
                    sample_rate = audio["sample_rate"]
                    sf.write(audio_path, waveform.T, sample_rate)
            except Exception as e:
                print(f"ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                print(f"Error saving audio to temporary file: {e}")
                audio_path = None
        
        # è§†é¢‘é¢„å¤„ç†
        # Video preprocessing
        video_frames = None
        if video_path:
            video_frames, video_fps, video_frames_count = self.preprocess_video(video_path)
            if video_frames is not None:
                print(f"è§†é¢‘å·²å¤„ç†: {video_path}, å¸§æ•°: {video_frames_count}, FPS: {video_fps}")
                print(f"Video processed: {video_path}, Frames: {video_frames_count}, FPS: {video_fps}")
        
        # æ„å»ºå¯¹è¯
        # Build conversation
        SYSTEM_PROMPT = "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."
        
        conversation = [
            {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
            {"role": "user", "content": []}
        ]
        
        # æ·»åŠ å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åˆ°å¯¹è¯
        # Add image, audio, and video to conversation
        if pil_image is not None:
            conversation[-1]["content"].append({"type": "image", "image": pil_image})
        
        use_video_audio = audio_source == "ğŸ¬ Video Built-in Audio"
        if audio_path and not use_video_audio:
            conversation[-1]["content"].append({"type": "audio", "audio": audio_path})
        
        if video_path and video_frames is not None:
            # è½¬æ¢è§†é¢‘å¸§ä¸ºPILå›¾åƒåˆ—è¡¨
            # Convert video frames to list of PIL images
            video_frame_list = []
            for frame in video_frames:
                frame = frame.permute(1, 2, 0).cpu().numpy() * 255
                frame = frame.astype(np.uint8)
                video_frame_list.append(Image.fromarray(frame))
            
            video_data = {
                "video": video_frame_list,
                "fps": video_fps,
                "total_frames": video_frames_count
            }
            conversation[-1]["content"].append({"type": "video", "video": video_frame_list})
            conversation[-1]["content"].append({"type": "video_data", "data": video_data})
        
        # å¤„ç†ç”¨æˆ·æç¤º
        # Process user prompt
        user_prompt = prompt if prompt.endswith(("?", ".", "ï¼", "ã€‚", "ï¼Ÿ", "ï¼")) else f"{prompt} "
        conversation[-1]["content"].append({"type": "text", "text": user_prompt})
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        # Apply chat template
        input_text = self.processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
        
        # å‡†å¤‡å¤„ç†å™¨å‚æ•°
        # Prepare processor parameters
        processor_args = {
            "text": input_text,
            "return_tensors": "pt",
            "padding": True,
            "use_audio_in_video": use_video_audio
        }
        
        # è°ƒç”¨å¤šæ¨¡æ€å¤„ç†é€»è¾‘
        # Call multimodal processing logic
        audios, images, videos = process_mm_info(conversation, use_audio_in_video=use_video_audio)
        processor_args["audio"] = audios
        processor_args["images"] = images
        processor_args["videos"] = videos
        
        # æ¸…ç†ä¸å†éœ€è¦çš„å¤§å¯¹è±¡
        # Clean up large objects that are no longer needed
        del video_frames, audios, images, videos
        torch.cuda.empty_cache()
        
        # åœ¨å‡½æ•°å¼€å§‹å¤„åˆå§‹åŒ–model_inputsä¸ºNone
        # Initialize model_inputs to None at the start of the function
        model_inputs = None
        
        # å°†è¾“å…¥ç§»è‡³è®¾å¤‡
        # Move inputs to device
        try:
            inputs = self.processor(**processor_args).to(self.model.device)
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            model_inputs = {
                k: v.to(self.device)
                for k, v in inputs.items()
                if v is not None
            }
            
            # ç¡®ä¿model_inputsåŒ…å«æ‰€éœ€çš„é”®
            # Ensure model_inputs contains required keys
            if "input_ids" not in model_inputs:
                raise ValueError("å¤„ç†åçš„è¾“å…¥ä¸åŒ…å«'input_ids'é”®")
                # ä¸­æ–‡æç¤º: å¤„ç†åçš„è¾“å…¥ä¸åŒ…å«'input_ids'é”®
                # Chinese prompt: Processed input does not contain 'input_ids' key
            
        except Exception as e:
            print(f"å¤„ç†è¾“å…¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            print(f"Error occurred while processing input: {e}")
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„é”™è¯¯å¤„ç†é€»è¾‘ï¼Œä¾‹å¦‚è¿”å›é»˜è®¤å€¼æˆ–æŠ›å‡ºç‰¹å®šå¼‚å¸¸
            # More error handling logic can be added here, such as returning default values or throwing specific exceptions
            raise RuntimeError("æ— æ³•å¤„ç†æ¨¡å‹è¾“å…¥") from e
            # ä¸­æ–‡æç¤º: æ— æ³•å¤„ç†æ¨¡å‹è¾“å…¥
            # Chinese prompt: Unable to process model input
        
        # ç”Ÿæˆé…ç½®
        # Generation configuration
        generate_config = {
            "max_new_tokens": max(max_tokens, 10),
            "temperature": temperature,
            "do_sample": True,
            "use_cache": True,
            "return_audio": audio_output != "ğŸ”‡None (No Audio)",
            "use_audio_in_video": use_video_audio,
            "top_p": top_p,
            "repetition_penalty": repetition_penalty,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id,
        }
        
        if generate_config["return_audio"]:
            generate_config["speaker"] = "Chelsie" if "Chelsie" in audio_output else "Ethan"
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        # Record GPU memory usage
        if torch.cuda.is_available():
            pre_forward_memory = torch.cuda.memory_allocated() / 1024**2
            print(f"ç”Ÿæˆå‰GPUå†…å­˜ä½¿ç”¨: {pre_forward_memory:.2f} MB")
            print(f"GPU memory usage before generation: {pre_forward_memory:.2f} MB")
        
        # æ£€æŸ¥model_inputsæ˜¯å¦å·²æ­£ç¡®åˆå§‹åŒ–
        # Check if model_inputs has been correctly initialized
        if model_inputs is None:
            raise RuntimeError("æ¨¡å‹è¾“å…¥æœªæ­£ç¡®åˆå§‹åŒ–")
            # ä¸­æ–‡æç¤º: æ¨¡å‹è¾“å…¥æœªæ­£ç¡®åˆå§‹åŒ–
            # Chinese prompt: Model inputs not initialized correctly

        # ä½¿ç”¨æ–°çš„autocast API
        # Use new autocast API
        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            outputs = self.model.generate(**model_inputs, **generate_config)

        # è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        # Record GPU memory usage
        if torch.cuda.is_available():
            post_forward_memory = torch.cuda.memory_allocated() / 1024**2
            print(f"ç”ŸæˆåGPUå†…å­˜ä½¿ç”¨: {post_forward_memory:.2f} MB")
            print(f"GPU memory usage after generation: {post_forward_memory:.2f} MB")
            print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­GPUå†…å­˜å¢åŠ : {post_forward_memory - pre_forward_memory:.2f} MB")
            print(f"GPU memory increase during generation: {post_forward_memory - pre_forward_memory:.2f} MB")
        
        # å¤„ç†è¾“å‡º
        # Process outputs
        if generate_config["return_audio"]:
            text_tokens = outputs[0] if outputs[0].dim() == 2 else outputs[0].unsqueeze(0)
            audio_tensor = outputs[1]
        else:
            text_tokens = outputs if outputs.dim() == 2 else outputs.unsqueeze(0)
            audio_tensor = torch.zeros(0, 0, device=self.model.device)
        
        # æ¸…ç†ä¸å†éœ€è¦çš„å¤§å¯¹è±¡
        # Clean up large objects that are no longer needed
        del outputs, inputs
        torch.cuda.empty_cache()
        
        # æˆªå–æ–°ç”Ÿæˆçš„token
        # Extract newly generated tokens
        input_length = model_inputs["input_ids"].shape[1]
        text_tokens = text_tokens[:, input_length:]  # æˆªå–æ–°ç”Ÿæˆçš„token
        # Extract newly generated tokens
        
        # è§£ç æ–‡æœ¬
        # Decode text
        text = self.tokenizer.decode(
            text_tokens[0],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        # ä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬ç”¨äºè°ƒè¯•
        # Save generated text for debugging
        self.last_generated_text = text
        del model_inputs
        torch.cuda.empty_cache()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        # Clean up temporary files
        if audio_path:
            try:
                os.remove(audio_path)
            except Exception as e:
                print(f"åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                print(f"Error deleting temporary audio file: {e}")
        
        # å¤„ç†éŸ³é¢‘è¾“å‡º
        # Process audio output
        if generate_config["return_audio"]:
            audio = audio_tensor
            if isinstance(audio, np.ndarray):
                audio = torch.from_numpy(audio).to(self.model.device)
            if audio.dim() == 1:
                audio = audio.unsqueeze(0)
        else:
            audio = torch.zeros(0, 0, device=self.model.device)
        
        if audio.dim() == 3:
            audio = audio.mean(dim=1)
        assert audio.dim() == 2, f"Audio waveform must be 2D, got {audio.dim()}D"
        # ä¸­æ–‡æç¤º: éŸ³é¢‘æ³¢å½¢å¿…é¡»æ˜¯2Dçš„ï¼Œå¾—åˆ°äº†{audio.dim()}D
        # Chinese prompt: Audio waveform must be 2D, got {audio.dim()}D
        
        audio_output_data = {
            "waveform": audio,
            "sample_rate": 24000
        }
        
        if generate_config["return_audio"]:
            buffer = io.BytesIO()
            torchaudio.save(buffer, audio_output_data["waveform"].cpu(), 24000, format="wav")
            buffer.seek(0)
            waveform, sample_rate = torchaudio.load(buffer)
            audio_output_data = {
                "waveform": waveform.unsqueeze(0),
                "sample_rate": sample_rate
            }
        
        # å†æ¬¡æ¸…ç†æ˜¾å­˜
        # Clean GPU memory again
        torch.cuda.empty_cache()
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        # Calculate processing time
        process_time = time.time() - start_time
        self.generation_stats["count"] += 1
        self.generation_stats["total_time"] += process_time
        
        # æ‰“å°æ€§èƒ½ç»Ÿè®¡
        # Print performance statistics
        print(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {process_time:.2f} ç§’")
        print(f"Generation completed, time taken: {process_time:.2f} seconds")
        if self.generation_stats["count"] > 0:
            avg_time = self.generation_stats["total_time"] / self.generation_stats["count"]
            print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f} ç§’/æ¬¡")
            print(f"Average generation time: {avg_time:.2f} seconds/time")
        
        return (text.strip(), audio_output_data)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (
                    list(MODEL_REGISTRY.keys()),  # åŠ¨æ€ç”Ÿæˆæ¨¡å‹é€‰é¡¹
                    # Dynamically generate model options
                    {
                        "default": next((name for name, info in MODEL_REGISTRY.items() if info.get("default", False)), 
                                       list(MODEL_REGISTRY.keys())[0]),
                        "tooltip": "Select the available model version.\né€‰æ‹©å¯ç”¨çš„æ¨¡å‹ç‰ˆæœ¬ã€‚"
                    }
                ),
                "quantization": (
                    [
                        "ğŸ‘ 4-bit (VRAM-friendly)",
                        "âš–ï¸ 8-bit (Balanced Precision)",
                        "ğŸš« None (Original Precision)"
                    ],
                    {
                        "default": "ğŸ‘ 4-bit (VRAM-friendly)",
                        "tooltip": "Select the quantization level:\nâœ… 4-bit: Significantly reduces VRAM usage, suitable for resource-constrained environments.\nâš–ï¸ 8-bit: Strikes a balance between precision and performance.\nğŸš« None: Uses the original floating-point precision (requires a high-end GPU).\n\né€‰æ‹©é‡åŒ–çº§åˆ«ï¼š\nâœ… 4ä½ï¼šæ˜¾è‘—å‡å°‘VRAMä½¿ç”¨ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚\nâš–ï¸ 8ä½ï¼šåœ¨ç²¾åº¦å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚\nğŸš« æ— ï¼šä½¿ç”¨åŸå§‹æµ®ç‚¹ç²¾åº¦ï¼ˆéœ€è¦é«˜ç«¯GPUï¼‰ã€‚"
                    }
                ),
                "prompt": (
                    "STRING",
                    {
                        "default": "Hi!ğŸ˜½",
                        "multiline": True,
                        "tooltip": "Enter a text prompt, supporting Chinese and emojis. Example: 'Describe a cat in a painter's style.'\nè¾“å…¥æ–‡æœ¬æç¤ºï¼Œæ”¯æŒä¸­æ–‡å’Œè¡¨æƒ…ç¬¦å·ã€‚ç¤ºä¾‹ï¼š'ä»¥ç”»å®¶çš„é£æ ¼æè¿°ä¸€åªçŒ«ã€‚'"
                    }
                ),
                "audio_output": (
                    [
                        "ğŸ”‡None (No Audio)",
                        "ğŸ‘±â€â™€ï¸Chelsie (Female)",
                        "ğŸ‘¨â€ğŸ¦°Ethan (Male)"
                    ],
                    {
                        "default": "ğŸ”‡None (No Audio)",
                        "tooltip": "Audio output options:\nğŸ”‡ Do not generate audio.\nğŸ‘±â€â™€ï¸ Use the female voice Chelsie (warm tone).\nğŸ‘¨â€ğŸ¦° Use the male voice Ethan (calm tone).\n\néŸ³é¢‘è¾“å‡ºé€‰é¡¹ï¼š\nğŸ”‡ ä¸ç”ŸæˆéŸ³é¢‘ã€‚\nğŸ‘±â€â™€ï¸ ä½¿ç”¨å¥³æ€§å£°éŸ³Chelsieï¼ˆæ¸©æš–è¯­è°ƒï¼‰ã€‚\nğŸ‘¨â€ğŸ¦° ä½¿ç”¨ç”·æ€§å£°éŸ³Ethanï¼ˆå¹³é™è¯­è°ƒï¼‰ã€‚"
                    }
                ),
                "audio_source": (
                    [
                        "ğŸ§ Separate Audio Input",
                        "ğŸ¬ Video Built-in Audio"
                    ],
                    {
                        "default": "ğŸ§ Separate Audio Input",
                        "display": "radio",
                        "tooltip": "Select audio source: Use video's built-in audio track (priority) / Input a separate audio file (external audio)\né€‰æ‹©éŸ³é¢‘æºï¼šä½¿ç”¨è§†é¢‘å†…ç½®éŸ³è½¨ï¼ˆä¼˜å…ˆï¼‰/è¾“å…¥å•ç‹¬çš„éŸ³é¢‘æ–‡ä»¶ï¼ˆå¤–éƒ¨éŸ³é¢‘ï¼‰"
                    }
                ),
                "max_tokens": (
                    "INT",
                    {
                        "default": 132,
                        "min": 64,
                        "max": 2048,
                        "step": 16,
                        "display": "slider",
                        "tooltip": "Control the maximum length of the generated text (in tokens). \nGenerally, 100 tokens correspond to approximately 50 - 100 Chinese characters or 67 - 100 English words, but the actual number may vary depending on the text content and the model's tokenization strategy. \nRecommended range: 64 - 512.\n\næ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥tokenä¸ºå•ä½ï¼‰ã€‚\nä¸€èˆ¬æ¥è¯´ï¼Œ100ä¸ªtokençº¦å¯¹åº”50-100ä¸ªæ±‰å­—æˆ–67-100ä¸ªè‹±æ–‡å•è¯ï¼Œä½†å®é™…æ•°é‡å¯èƒ½å› æ–‡æœ¬å†…å®¹å’Œæ¨¡å‹çš„åˆ†è¯ç­–ç•¥è€Œå¼‚ã€‚\næ¨èèŒƒå›´ï¼š64-512ã€‚"
                    }
                ),
                "temperature": (
                    "FLOAT",
                    {
                        "default": 0.4,
                        "min": 0.1,
                        "max": 1.0,
                        "step": 0.1,
                        "display": "slider",
                        "tooltip": "Control the generation diversity:\nâ–«ï¸ 0.1 - 0.3: Generate structured/technical content.\nâ–«ï¸ 0.5 - 0.7: Balance creativity and logic.\nâ–«ï¸ 0.8 - 1.0: High degree of freedom (may produce incoherent content).\n\næ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ï¼š\nâ–«ï¸ 0.1-0.3ï¼šç”Ÿæˆç»“æ„åŒ–/æŠ€æœ¯æ€§å†…å®¹ã€‚\nâ–«ï¸ 0.5-0.7ï¼šå¹³è¡¡åˆ›é€ æ€§å’Œé€»è¾‘æ€§ã€‚\nâ–«ï¸ 0.8-1.0ï¼šé«˜åº¦è‡ªç”±ï¼ˆå¯èƒ½äº§ç”Ÿä¸è¿è´¯å†…å®¹ï¼‰ã€‚"
                    }
                ),
                "top_p": (
                    "FLOAT",
                    {
                        "default": 0.9,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "display": "slider",
                        "tooltip": "Nucleus sampling threshold:\nâ–ªï¸ Close to 1.0: Retain more candidate words (more random).\nâ–ªï¸ 0.5 - 0.8: Balance quality and diversity.\nâ–ªï¸ Below 0.3: Generate more conservative content.\n\næ ¸é‡‡æ ·é˜ˆå€¼ï¼š\nâ–ªï¸ æ¥è¿‘1.0ï¼šä¿ç•™æ›´å¤šå€™é€‰è¯ï¼ˆæ›´éšæœºï¼‰ã€‚\nâ–ªï¸ 0.5-0.8ï¼šå¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§ã€‚\nâ–ªï¸ ä½äº0.3ï¼šç”Ÿæˆæ›´ä¿å®ˆçš„å†…å®¹ã€‚"
                    }
                ),
                "repetition_penalty": (
                    "FLOAT",
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 2.0,
                        "step": 0.01,
                        "display": "slider",
                        "tooltip": "Control of repeated content:\nâš ï¸ 1.0: Default behavior.\nâš ï¸ >1.0 (Recommended 1.2): Suppress repeated phrases.\nâš ï¸ <1.0 (Recommended 0.8): Encourage repeated emphasis.\n\næ§åˆ¶é‡å¤å†…å®¹ï¼š\nâš ï¸ 1.0ï¼šé»˜è®¤è¡Œä¸ºã€‚\nâš ï¸ >1.0ï¼ˆæ¨è1.2ï¼‰ï¼šæŠ‘åˆ¶é‡å¤çŸ­è¯­ã€‚\nâš ï¸ <1.0ï¼ˆæ¨è0.8ï¼‰ï¼šé¼“åŠ±é‡å¤å¼ºè°ƒã€‚"
                    }
                )
            },
            "optional": {
                "image": (
                    "IMAGE",
                    {
                        "tooltip": "Upload a reference image (supports PNG/JPG), and the model will adjust the generation result based on the image content.\nä¸Šä¼ å‚è€ƒå›¾åƒï¼ˆæ”¯æŒPNG/JPGï¼‰ï¼Œæ¨¡å‹å°†æ ¹æ®å›¾åƒå†…å®¹è°ƒæ•´ç”Ÿæˆç»“æœã€‚"
                    }
                ),
                "audio": (
                    "AUDIO",
                    {
                        "tooltip": "Upload an audio file (supports MP3/WAV), and the model will analyze the audio content and generate relevant responses.\nä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒMP3/WAVï¼‰ï¼Œæ¨¡å‹å°†åˆ†æéŸ³é¢‘å†…å®¹å¹¶ç”Ÿæˆç›¸å…³å“åº”ã€‚"
                    }
                ),
                "video_path": (
                    "VIDEO_PATH",
                    {
                        "tooltip": "Enter the video file  (supports MP4/WEBM), and the model will extract visual features to assist in generation.\nè¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒMP4/WEBMï¼‰ï¼Œæ¨¡å‹å°†æå–è§†è§‰ç‰¹å¾è¾…åŠ©ç”Ÿæˆã€‚"
                    }
                )
            }
        }

    RETURN_TYPES = ("STRING", "AUDIO")
    RETURN_NAMES = ("text", "audio")
    FUNCTION = "process"
    CATEGORY = "ğŸ¼QwenOmni"    


NODE_CLASS_MAPPINGS = {
    "VideoUploader": VideoUploader,
    "QwenOmniCombined": QwenOmniCombined
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoUploader": "Video UploaderğŸ¼",
    "QwenOmniCombined": "Qwen Omni CombinedğŸ¼"
}
